#!/bin/bash

# Get project name from directory name
PROJECT_NAME=$(basename $(pwd))
ENV_NAME=$(echo $PROJECT_NAME | tr '-' '_')

echo "Setting up AWS project: $PROJECT_NAME"

# Ask about conda environment
read -p "Create a dedicated conda environment? (y/n): " CREATE_ENV
if [[ "$CREATE_ENV" == "y" ]]; then
    # Try to source conda activation script
    CONDA_BASE=$(conda info --base 2>/dev/null)
    if [[ -n "$CONDA_BASE" && -f "$CONDA_BASE/etc/profile.d/conda.sh" ]]; then
        source "$CONDA_BASE/etc/profile.d/conda.sh"
        echo "Sourced conda.sh from $CONDA_BASE"
    elif [[ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]]; then
        source "$HOME/miniconda3/etc/profile.d/conda.sh"
        echo "Sourced conda.sh from $HOME/miniconda3"
    elif [[ -f "$HOME/anaconda3/etc/profile.d/conda.sh" ]]; then
        source "$HOME/anaconda3/etc/profile.d/conda.sh"
        echo "Sourced conda.sh from $HOME/anaconda3"
    else
        echo "WARNING: Could not automatically source conda.sh. Assuming 'conda' command is available."
    fi
    
    echo "Creating conda environment: $ENV_NAME ..."
    conda create -y -n $ENV_NAME python=3.9
    conda activate $ENV_NAME
    
    # Install AWS dependencies
    echo "Installing dependencies..."
    conda install -y numpy pandas matplotlib jupyter ipykernel
    pip install boto3 awscli metaflow sagemaker

    echo "# Generated by conda environment creation" > environment.yml
    conda env export > environment.yml

    echo "Created conda environment: $ENV_NAME with AWS dependencies"
else
    echo "Skipping conda environment creation. Please install required packages manually."
fi

# Create modern ML project structure with AWS integrations
mkdir -p src/$PROJECT_NAME/{data,models,utils,visualization,cloud}
mkdir -p notebooks tests configs data results docs

# Create __init__.py files to make the modules importable
touch src/$PROJECT_NAME/__init__.py
touch src/$PROJECT_NAME/data/__init__.py
touch src/$PROJECT_NAME/models/__init__.py
touch src/$PROJECT_NAME/utils/__init__.py
touch src/$PROJECT_NAME/visualization/__init__.py
touch src/$PROJECT_NAME/cloud/__init__.py

# Create setup.py for easier importing
cat > setup.py << EOL
from setuptools import setup, find_packages

setup(
    name="$PROJECT_NAME",
    version="0.1",
    packages=find_packages(where="src"),
    package_dir={"": "src"},
)
EOL

# Create AWS config directory
mkdir -p .aws
touch .aws/credentials.template
cat > .aws/credentials.template << EOL
[default]
aws_access_key_id = YOUR_ACCESS_KEY
aws_secret_access_key = YOUR_SECRET_KEY
region = us-east-1
EOL
echo ".aws/" >> .gitignore

# Setup Metaflow configuration
mkdir -p .metaflow
cat > .metaflow/config.json << EOL
{
    "METAFLOW_PROFILE": "aws",
    "METAFLOW_DEFAULT_DATASTORE": "s3",
    "METAFLOW_DEFAULT_METADATA": "service",
    "METAFLOW_S3_BUCKET_NAME": "phd-$PROJECT_NAME-metaflow"
}
EOL
echo ".metaflow/" >> .gitignore

# Create basic files
touch requirements.txt
cat > requirements.txt << EOL
# Core Data Science
numpy
pandas
matplotlib
scikit-learn

# AWS & Deployment
boto3
metaflow>=2.8.0
sagemaker
aws-cdk-lib
constructs>=10.0.0

# Notebooks
jupyter
ipykernel

# Development
pytest
black
flake8

# Add your additional requirements here
EOL

# Create README with AWS-focused documentation
cat > README.md << EOL
# $PROJECT_NAME

## AWS Setup
1. Configure AWS credentials:
   \`\`\`bash
   cp .aws/credentials.template .aws/credentials
   # Edit .aws/credentials with your AWS access keys
   \`\`\`
2. Set up S3 buckets (edit region/bucket names if needed):
   \`\`\`bash
   ./setup_aws.sh
   \`\`\`

## Environment Setup

*This setup assumes you have already cloned the repository.*

\`\`\`bash
cd $PROJECT_NAME # Navigate to project directory if not already there

# Option 1: Create and activate conda environment (Recommended)
# (Run this if you didn't create the environment during initial setup)
conda env create -f environment.yml
conda activate $ENV_NAME

# Option 2: Install in existing environment
# Ensure your current environment has the necessary packages
pip install -e .
pip install -r requirements.txt
\`\`\`

## Project Structure
\`\`\`
├── .aws/             # AWS credential templates (not in Git)
├── .metaflow/        # Metaflow configuration
├── configs/          # Configuration files
├── data/             # Raw and processed data (not in Git)
├── notebooks/        # Jupyter notebooks
├── results/          # Output of experiments (not in Git)
├── src/              # Source code
│   └── $PROJECT_NAME/
│       ├── cloud/    # AWS deployment code
│       ├── data/     # Data processing
│       ├── models/   # ML models
│       ├── utils/    # Utilities
│       └── visualization/ # Plotting and visualization
└── tests/            # Unit tests
\`\`\`

## Using Metaflow with AWS
\`\`\`python
from metaflow import FlowSpec, step, S3

class MyFlow(FlowSpec):
    @step
    def start(self):
        self.next(self.process_data)
        
    @step
    def process_data(self):
        """Process the data."""
        import numpy as np
        self.data = np.random.random(10)
        self.next(self.store_results)
        
    @step
    def store_results(self):
        """Store results to S3."""
        import pickle
        # Store to S3 when running on AWS
        with S3(run=self) as s3:
            s3.put("results.pkl", pickle.dumps(self.data))
        self.next(self.end)
        
    @step
    def end(self):
        """End the flow."""
        print("Flow complete!")

if __name__ == '__main__':
    MyFlow()
\`\`\`

## License
Copyright (c) $(date +%Y) [Your Name/Organization]
EOL

# Initialize git repository if it doesn't exist (for local-only scenario)
if [ ! -d .git ]; then
    git init
    echo "Initialized local git repository."
fi

# Add common ignores to .gitignore
echo "
# Standard Python ignores
__pycache__/
*.pyc
*.pyo
*.pyd
*.egg-info/
dist/
build/

# Data & Results
data/
results/

# IDE & Environment specific
.vscode/
.idea/
.ipynb_checkpoints/

# Secrets
.aws/credentials
*.env" >> .gitignore

# Create config template
mkdir -p configs
cat > configs/default.yml << EOL
# Project configuration
project_name: $PROJECT_NAME
aws:
  region: us-east-1
  s3_bucket: phd-$PROJECT_NAME-data
  batch_compute_environment: myjobenv
data_path: data/
results_path: results/
EOL

# Create AWS setup script
cat > setup_aws.sh << EOL
#!/bin/bash
echo "Setting up AWS resources for $PROJECT_NAME..."

# Load config
REGION=$(grep 'region:' configs/default.yml | cut -d' ' -f2)
DATA_BUCKET=$(grep 's3_bucket:' configs/default.yml | cut -d' ' -f2)
METAFLOW_BUCKET=$(grep 'METAFLOW_S3_BUCKET_NAME:' .metaflow/config.json | cut -d'"' -f4)
RESULTS_BUCKET="phd-$PROJECT_NAME-results" # Define results bucket name

echo "Using region: $REGION"
echo "Data bucket: $DATA_BUCKET"
echo "Metaflow bucket: $METAFLOW_BUCKET"
echo "Results bucket: $RESULTS_BUCKET"

# Create S3 buckets
aws s3 mb s3://$DATA_BUCKET --region $REGION
aws s3 mb s3://$RESULTS_BUCKET --region $REGION
aws s3 mb s3://$METAFLOW_BUCKET --region $REGION

echo "AWS S3 buckets created (or already exist)."

# Optional: Configure Metaflow to use AWS 
# metaflow configure aws # Usually done once globally
EOL
chmod +x setup_aws.sh

# Create example Metaflow script
mkdir -p src/$PROJECT_NAME/cloud
cat > src/$PROJECT_NAME/cloud/example_flow.py << EOL
"""Example Metaflow workflow."""
from metaflow import FlowSpec, step, S3, batch, conda_base, Parameter

@conda_base(libraries={'scikit-learn': '1.0.2', 'pandas': '1.4.1'})
class ${ENV_NAME}Flow(FlowSpec):
    """
    A simple Metaflow for $PROJECT_NAME.
    
    Run locally:
    python src/$PROJECT_NAME/cloud/example_flow.py run
    
    Run on AWS Batch:
    python src/$PROJECT_NAME/cloud/example_flow.py --with batch run
    """
    
    data_size = Parameter('data_size', default=100, type=int, help='Size of the random dataset')

    @step
    def start(self):
        """Generate sample data."""
        import numpy as np
        self.data = np.random.rand(self.data_size, 5)
        self.labels = (np.sum(self.data, axis=1) > 2.5).astype(int)
        print(f"Generated data with shape: {self.data.shape}")
        self.next(self.train_model)
        
    @batch(cpu=2, memory=4000) # Decorator to run this step on AWS Batch
    @step
    def train_model(self):
        """Train a simple model."""
        from sklearn.linear_model import LogisticRegression
        self.model = LogisticRegression()
        self.model.fit(self.data, self.labels)
        print(f"Model trained: {self.model}")
        self.next(self.evaluate_model)
        
    @step
    def evaluate_model(self):
        """Evaluate the model (example)."""
        from sklearn.metrics import accuracy_score
        predictions = self.model.predict(self.data)
        self.accuracy = accuracy_score(self.labels, predictions)
        print(f"Model accuracy: {self.accuracy:.3f}")
        self.next(self.store_results)
        
    @step
    def store_results(self):
        """Store results to S3."""
        import pickle
        # Store to S3 when running on AWS
        results = {
            'accuracy': self.accuracy,
            'model': self.model
        }
        with S3(run=self) as s3:
            results_path = f"results/run_{self.run_id}_results.pkl"
            s3.put(results_path, pickle.dumps(results))
            print(f"Results stored to S3 at: {s3.path(results_path)}")
        self.next(self.end)
        
    @step
    def end(self):
        """End the flow."""
        print("Flow finished!")

if __name__ == '__main__':
    ${ENV_NAME}Flow()
EOL

echo "AWS project setup complete!" 